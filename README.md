# 키워드로 알아보는 정치 이슈 분석
약 2주동안 진행된 프로젝트에서 문서 요약 모델인 KoBART 모델 개발 및 모델 배포(with Flask), 데이터 전처리, Mongo DB에 적재, 프레젠테이션의 역할을 수행했다.

[프로젝트 내용 설명 영상](https://drive.google.com/file/d/13q-2o8an19rO_ubByiH0TUJkpsyLO0IN/view?usp=sharing)

## 프로젝트 개요 및 필요성

- 네이버 뉴스는 800여개의 언론사에서 일 평균 6만여건의 기사가 생성되고, 2019년부터 누적된 기사수를 집계하면 약 1억 3천만건의 기사가 존재한다. 

- 바쁜 현대 사회에서 기사의 모든 글을 다 읽는 것보다는 요약문을 통해 기사의 핵심만 파악하고, 요약문이 흥미롭다면 기사 원문을 자세히 읽어보는 것은 어떨까라는 생각에서 출발했다.

- 뉴스 댓글에서 형태소를 추출하여 해당 댓글이 달린 기사에서 많이 언급된 단어를 워드 클라우드로 표현하고, 댓글이나 기사를 감성 분석 한다면 단순한 문서 요약 뿐만 아니라 하나의 기사에서도 다양한 정보들을 사용자에게 보여줄 수 있다는 점에서 유용한 서비스가 될 것이라고 기대했다.

## 프로젝트 파이프라인
1) 네이버 뉴스에서 기사와 댓글 데이터를 수집하고, 수집된 데이터를 Mongo DB에 적재. 

2) 적재된 데이터는 KoELECTRA 모델을 사용하여 댓글과 제목에 대한 감성 분석을 진행하고 예측 결과를 다시 DB에 저장하여 업데이트.

3) 감성 분석 결과가 저장된 데이터를 검색엔진인 Elastic Search에 맞는 문법으로 변환. 

4) 불용어, 유의어, 고유명사 사전을 인덱싱해서 더 효율적인 검색엔진 구현.

한 줄 정리: Elastic Search 엔진에 사용자가 키워드를 검색하면 백엔드와, 프론트엔드 서버에서 해당 키워드에 대한 워드 클라우드, 감성분석 등의 결과를 보여주고, 미리 개발한 KOBART 모델을 Flask로 배포시켜서 사용자에게 키워드와 관련된 기사들을 요약해주는 서비스.

- Elastic Search -> 처음에는 MongoDB 자체에서 검색할 방법을 고민했는데 MongoDB는 한글에 적합한 n-gram 인덱싱을 지원하지 않았다. 반면 엘라스틱 서치는 n-gram 뿐 아니라 nori라는 한국어 전용 형태소 분석기를 지원하는 nosql 기반 검색엔진으로, MongoDB와 마찬가지로 집계 함수를 지원하여 속도가 매우 빠르다는 장점이 있어서 사용했다.

![image](https://user-images.githubusercontent.com/97672187/178143110-60ae151d-ffce-4135-9113-df504929d519.png)

## 데이터 설명
1) 서비스에 사용한 데이터(네이버 뉴스 - 정치 일반)

- 수집 날짜 : 2019년 12월 ~ 2022년 1월

- 수집 데이터 : 뉴스 타이틀, 본문, 댓글

- 데이터 총 용량 : 20GB 

- 뉴스 개수 : 2천만개

- 댓글 개수 : 1억개

2) Fine Tuning

- KoBERT and KoELECTRA

![image](https://user-images.githubusercontent.com/97672187/178143684-c8a11c00-f971-44ca-95a3-d5d710ca2407.png)

네이버 영화 리뷰 데이터 + AI-Hub 일상 대화 감성 데이터: train dataset 약 20만건 / test dataset 약 6만건

- **KoBART**

![image](https://user-images.githubusercontent.com/97672187/178143729-96176e33-2526-4098-8bd8-741783af2fe3.png)

AI-Hub 문서요약 텍스트: train dataset 약 30만건 / test dataset 약 3만건

## 모델링
1) KoBERT

- 2018년 기준 11개의 자연어 처리 문제에서 SOTA를 달성한 구글의 사전학습 모델 BERT의 한국어 특화 모델

- 기존의 사전학습 모델(GPT-1, ELMo 등)과 달리 양방향성(Bidirectional)을 가짐

- 준수한 성능과 풍부한 레퍼런스가 존재

2) KoELECTRA

- 기존 MLM(Masked Language Model)의 학습율이 낮다는 문제점을 해결하기 위해 구글에서 2020년에 개발한 사전 학습 모델

- GPT-2로 120일 동안 진행한 학습을 ELECTRA로 4일만에 해결함

- 본 프로젝트에서 감성 분석을 위해 가장 처음 사용했던 모델인 KoBERT는 모델의 용량이 400MB, 100개의 문장 처리 속도가 10초였는데 KoELECTRA 모델은 54MB, 100개 문장 처리 속도가
1초로 비슷한 성능임에도 훨씬 더 빠른 속도를 자랑했다.

    -> 따라서 최종적으로 KoELECTRA 모델을 뉴스 타이틀 및 댓글의 감성분석 모델로 사용했다.

3) **KoBART**

- BERT의 인코더와 GPT의 디코더 부분을 결합한 Transformers 모델 BART를 한국어로 특화시킨 모델

- 40GB 이상의 한국어 텍스트 데이터를 사전학습시켜 기계 번역, 감성분석, 챗봇, **문서 요약** 등의 태스크에서 사용가능

- 인코더와 디코더가 모두 존재해서 양방향의 문맥정보를 반영하고, BERT의 특징 중 하나인 MLM을 사용하여 노이즈에 유연하다는 장점이 있다.

- 특히 요약 부분에서 다른 모델들에 비해 우수한 성능을 자랑해서 본 프로젝트의 정치 기사 요약파트의 최종 모델로 선정했다.

4) **KoBART** 모델링 과정 및 성능

KoBART 모델은 AI Hub 문서요약 데이터셋을 사용했고, 일반화 능력을 향상시키기 위해 기존 30만개의 데이터에 전처리된 데이터를 20만개 정도 추가해서 총 50만개의 데이터로 파인 튜닝을 시도했다. 하지만, 이미 무거운 모델이다 보니 학습 시간이 매우 오래 걸릴 뿐만 아니라 GPU 환경도 좋지 못해서 결국 50만개의 데이터에서 학습 데이터는 랜덤하게 2만개의 데이터를 추출하고, 검증 데이터는 5천개의 데이터를 추출하여 더 작은 데이터로 파인튜닝을 진행했다. Batch size, Learning rate, Epochs 등의 하이퍼 파라미터를 조정한 결과, 문서 요약 task에서 흔하게 사용되는 지표중 하나인 rouge-1 이라는 평가 지표에서 준수한 0.3 score를 기록했다.

## 시연영상

[시연영상 링크](https://drive.google.com/file/d/1HuZTil5fKnk5nqwfpHlzRhtSRQEkLIx2/view?usp=sharing)

## 한계점 및 해결방안
- 기존 계획에는 네이버를 포함한 여러 커뮤니티 데이터 그리고 정치 키워드 외에도 여러 키워드를 사용하고자 했으나 DB에 데이터를 적재하는 시간과 서버 비용으로 인하여 네이버 뉴스 정치 섹션 데이터만 사용하게 되었다.

    -> 시간이 더 주어진다면 다양한 커뮤니티에서 훨씬 더 많은 데이터를 수집하고, 커뮤니티나 언론사별로 키워드에 따른 검색 결과를 비교하여 더 흥미로운 정보를 사용자에게 제공해줄 수 있을 것 같다.

- 문서 요약 모델의 Fine tuning 시간이 오래 걸려서 많은 데이터를 사용하지 못했다.

    -> Colab이 아닌 안정적이고 좋은 서버환경에서 GPU를 사용하여 모델을 학습시켰다면 더 성능이 높은 문서 요약 모델을 개발할 수 있었을 것 같다.

- 감성분석에 중립 클래스를 사용하고 싶었지만, 중립 데이터는 긍/부정 데이터에 비해 데이터의 양이 상대적으로 작은 불균형 데이터였다.

    -> 기존 데이터에는 중립 클래스가 적었기 때문에 중립으로 라벨링된 텍스트 데이터를 추가적으로 수집한다면 데이터 불균형을 해소할 수 있을 것이다.

## 개발환경

![image](https://user-images.githubusercontent.com/97672187/178144400-c4f63f9f-cbd1-4140-bd98-b19486267a33.png)


